# -*- coding: utf-8 -*-
"""text analysis2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GDFupZJJBPP-8z9rjOH38oHp7A84BWjn
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import numpy as np
import pandas as pd
import statistics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from google.colab import drive
drive.mount('/content/drive')

!pip install wordcloud
from wordcloud import WordCloud, STOPWORDS
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# %matplotlib inline


import os
import re
import nltk
from nltk.corpus import stopwords # Import stopwords from nltk.corpus

# laod data and show information about it
submet=pd.read_csv('/content/drive/MyDrive/data set A2/submit.csv')

test=pd.read_csv('/content/drive/MyDrive/data set A2/train[1].csv')

train=pd.read_csv('/content/drive/MyDrive/data set A2/test[1].csv')
print(train.shape,test.shape, submet.shape)
train.head()

#check the missing data
print(train.isnull().sum())
print('------------------')
print(test.isnull().sum())
print('------------------')
print(submet.isnull().sum())
# handle the missing data
test=test.fillna(' ')
train=train.fillna(' ')
test['total']=test['title']+' '+test['author']+' '+test['text']
train['total']=train['title']+' '+train['author']+' '+train['text']

#
#WORDCLOUD

real_words = ''
fake_words = ''
stopwords = set(STOPWORDS)

#iterate through the csv file
for val in train[submet['label']==0].total:
    tokens = val.split()
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    real_words += " ".join(tokens)+" "
for val in train[submet['label']==1].total:
    tokens = val.split()
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    fake_words += " ".join(tokens)+ " "

wordcloud = WordCloud(width = 1000,height=1000,
                      background_color = 'white',
                      stopwords = stopwords,
                      min_font_size = 10).generate(real_words)

#Plot the real words wordcloud image
plt.figure(figsize = (10,10), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

# pre Preprocessing
#TOKENIZATION
#Downloading nltk data
nltk.download('punkt')
nltk.word_tokenize("Hello how are you")
#STOPWORDS
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words = stopwords.words('english')
print(stop_words)
stop_words.append('')
print(stop_words)

#LEMMATIZATION
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')

# Assuming 'id' is the common column in both DataFrames
merged_df = pd.merge(train, submet[['id', 'label']], on='id', how='left')
#APPLICATION
lemmatizer = WordNetLemmatizer()
for index,row in merged_df.iterrows():
    filter_sentence = ''

    sentence = row['total']
    sentence = re.sub(R'[^\w\s]','',sentence)  #cleaning

    words = nltk.word_tokenize(sentence)  #tokenization

    words = [w for w in words if not w in stop_words] #stopwords removal

    for word in words:
        filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()

    merged_df.loc[index,'total'] = filter_sentence

    train = merged_df[['total','label']]

train.head()

#split the data
Train_X = train['total']
Train_Y = train['label']
Test_X = test['total']
Test_Y = test['label']

from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score

Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(train['total'],train['label'],test_size=0.3)

from sklearn.feature_extraction.text import TfidfVectorizer

Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(train['total'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

print(Tfidf_vect.vocabulary_)

# fit the training dataset on the NB classifier
Naive = naive_bayes.MultinomialNB()
Naive.fit(Train_X_Tfidf,Train_Y)
# predict the labels on validation dataset
predictions_NB = Naive.predict(Test_X_Tfidf)
# Use accuracy_score function to get the accuracy
print("Naive Bayes Accuracy Score -> ",accuracy_score(predictions_NB, Test_Y)*100)
# print the classification report
print(classification_report(predictions_NB, Test_Y))

#Classifier - Algorithm - SVM
# fit the training dataset on the classifier
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
SVM.fit(Train_X_Tfidf,Train_Y)
# predict the labels on validation dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)
# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score -> ",accuracy_score(predictions_SVM, Test_Y)*100)
print(classification_report(predictions_SVM, Test_Y))